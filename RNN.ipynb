{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Using a Numpy Implementation of a Recurrent Neural Network to Generate New Trump Speeches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sigmoid(x, deriv=False):\n",
    "    if not deriv:\n",
    "        return 1 / (1 + np.exp(-x))\n",
    "    else:\n",
    "        return np.exp(x) / ((np.exp(x) + 1) **2)\n",
    "    \n",
    "# Softmax function for output probabilities\n",
    "def softmax(x):\n",
    "    nonlin_x = np.exp(x)\n",
    "    return nonlin_x / np.sum(nonlin_x, 1).reshape(nonlin_x.shape[0], 1)\n",
    "\n",
    "class RecurrentNeuralNet:\n",
    "    def __init__(self, learning_rate, input_dim, hidden_dim, output_dim):\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.input_dim = input_dim\n",
    "        self.learning_rate = learning_rate\n",
    "        \n",
    "        self.l1weights = np.random.randn(input_dim, hidden_dim) * 0.01\n",
    "        self.l1biases = np.random.randn(1, hidden_dim) * 0.01\n",
    "        self.umatrix = np.random.randn(hidden_dim, hidden_dim) * 0.01\n",
    "        self.l2weights = np.random.randn(hidden_dim, output_dim) * 0.01\n",
    "        self.l2biases = np.random.randn(1, output_dim) * 0.01\n",
    "        \n",
    "        # Adagrad memory will be used to apply adaptive gradients, adjusting the\n",
    "        # learning rate based on the parameters.\n",
    "        self.adagrad_memory = (np.zeros_like(self.l1weights), np.zeros_like(self.l1biases), \n",
    "                       np.zeros_like(self.umatrix), np.zeros_like(self.l2weights), \n",
    "                       np.zeros_like(self.l2biases))\n",
    "    \n",
    "    # Getting a generated sample by predicting the next in a sequence\n",
    "    # over and over again, and adding the prediction to the sequence.\n",
    "    def sample(self, prev_hidden, startchar, length=200):\n",
    "        Xs = [startchar]\n",
    "        prev_char = startchar\n",
    "        \n",
    "        prev_hidden = np.zeros([1, self.hidden_dim])\n",
    "        for i in range(length):\n",
    "            X = np.zeros([1, self.input_dim])\n",
    "            X[:,prev_char] = 1\n",
    "            \n",
    "            # Forward pass\n",
    "            hidden = sigmoid(np.dot(X, self.l1weights) + np.dot(prev_hidden, self.umatrix) + self.l1biases)\n",
    "            prev_hidden = hidden\n",
    "            prob = softmax(np.dot(hidden, self.l2weights) + self.l2biases)\n",
    "            \n",
    "            # Sample from output probability distribution\n",
    "            new_x = np.random.choice(range(self.input_dim), p=prob.ravel())\n",
    "            \n",
    "            Xs.append(new_x)\n",
    "            prev_char = new_x\n",
    "            \n",
    "        return Xs\n",
    "    \n",
    "    # Computes the gradients of all the parameters in the network\n",
    "    # using the chain rule and backpropagation through time.\n",
    "    def compute_gradients(self, X, Y, prev_h):\n",
    "        \n",
    "        # Starts with memory from the previous sequence due to\n",
    "        # the training sequence starting in the middle of the\n",
    "        # txt file\n",
    "        prev_hidden = np.copy(prev_h)\n",
    "        \n",
    "        # Initializing buffers for pre-activations and activations\n",
    "        # at every timestep\n",
    "        Yhat = []\n",
    "        Pre_Yhat = []\n",
    "        H = []\n",
    "        Pre_H = []\n",
    "        \n",
    "        # Initializing gradient buffers\n",
    "        l1weights_grads = []\n",
    "        l1biases_grads = []\n",
    "        umatrix_grads = []\n",
    "        l2weights_grads = []\n",
    "        l2biases_grads = []\n",
    "        \n",
    "        # Forward pass\n",
    "        for i in range(len(X)):\n",
    "            # Computes the value of the current timestep, multiplying the\n",
    "            # umatrix and the hidden state at the previous timestep and\n",
    "            # adding it on to the product of the input and the first weight\n",
    "            # matrix.\n",
    "            pre_hidden = np.dot(X[i:i+1], self.l1weights) + np.dot(prev_hidden, self.umatrix) + self.l1biases\n",
    "            hidden = sigmoid(pre_hidden)\n",
    "            \n",
    "            # Storing the current hidden state to be used in the next\n",
    "            # timestep.\n",
    "            prev_hidden = hidden\n",
    "            \n",
    "            pre_output = np.dot(hidden, self.l2weights) + self.l2biases\n",
    "            output = softmax(pre_output)\n",
    "            \n",
    "            Pre_H.append(pre_hidden)\n",
    "            H.append(hidden)\n",
    "            Pre_Yhat.append(pre_output)\n",
    "            Yhat.append(output)\n",
    "            \n",
    "        # Backward pass\n",
    "        \n",
    "        # Since the loss is not computer beyond the last timestep, we can\n",
    "        # set the gradient of the hidden state at the next timestep to 0\n",
    "        next_Pre_H_grad = np.zeros([1, self.hidden_dim])\n",
    "        \n",
    "        # Gradients will be calculated for every timestep going backwards,\n",
    "        # since the gradient for the hidden layer at the next timestep\n",
    "        # is necessary for computing the gradient of the hidden layer at\n",
    "        # the next timestep\n",
    "        for i in reversed(range(len(X))):\n",
    "            \n",
    "            # Finding the gradient of softmax crossentropy w.r.t logits\n",
    "            Pre_Yhat_grad = Yhat[i] - Y[i]\n",
    "            \n",
    "            # Gradient of the hidden layer activations, where the gradient of\n",
    "            # the previous hidden state is added, since loss is computed at\n",
    "            # every timestep\n",
    "            H_grad = np.dot(Pre_Yhat_grad, self.l2weights.T) + np.dot(next_Pre_H_grad, self.umatrix.T)\n",
    "            \n",
    "            # Gradient of hidden layer pre-activations\n",
    "            Pre_H_grad = H_grad * sigmoid(Pre_H[i], deriv=True)\n",
    "            \n",
    "            # Storing this gradient to be used in the previous timestep\n",
    "            next_Pre_H_grad = Pre_H_grad\n",
    "            \n",
    "            # Compute the gradients for the weights and biases in the second layer\n",
    "            l2weights_grads.append(np.dot(H[i].T, Pre_Yhat_grad))\n",
    "            l2biases_grads.append(Pre_Yhat_grad)\n",
    "            \n",
    "            # Compute the gradients for the umatrix, connecting the hidden state\n",
    "            # of the previous timestep and the current timestep\n",
    "            if i != 0:\n",
    "                umatrix_grads.append(np.dot(H[i-1].T, next_Pre_H_grad))\n",
    "            \n",
    "            # Computer the gradients for the weights and biases in the first layer\n",
    "            l1weights_grads.append(np.dot(X[i:i+1].T, Pre_H_grad))\n",
    "            l1biases_grads.append(Pre_H_grad)\n",
    "            \n",
    "        # Average the gradients for all parameters over all timesteps\n",
    "        l1weight_grad = np.mean(np.array(l1weights_grads), 0)\n",
    "        l1bias_grad = np.mean(np.array(l1biases_grads), 0)\n",
    "        umatrix_grad = np.mean(np.array(umatrix_grads), 0)\n",
    "        l2weight_grad = np.mean(np.array(l2weights_grads), 0)\n",
    "        l2bias_grad = np.mean(np.array(l2biases_grads), 0)\n",
    "        \n",
    "        return (l1weight_grad, l1bias_grad, umatrix_grad, l2weight_grad, l2bias_grad), prev_hidden\n",
    "    \n",
    "    # Applying the adaptive gradient descent\n",
    "    # optimizer to parameters given gradients\n",
    "    def apply_adagrad(self, gradients):\n",
    "        \n",
    "        # Iterate through parameter matrices with memory\n",
    "        # and gradients\n",
    "        for theta, mem, grad in zip((self.l1weights, self.l1biases, self.umatrix, self.l2weights, self.l2biases),\n",
    "                                    self.adagrad_memory, gradients):\n",
    "            \n",
    "            # Add squared gradients element-wise to\n",
    "            # the memory, increasing it over time\n",
    "            mem += grad * grad\n",
    "            \n",
    "            # Decaying learning rate over time as memory\n",
    "            # values increase\n",
    "            theta += -self.learning_rate * grad / np.sqrt(mem + 1e-8)\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preprocessing\n",
    "\n",
    "text = open(\"speeches.txt\", 'r').read()\n",
    "\n",
    "# Creating a list that contains every unique character\n",
    "in_to_char = list(set(text))\n",
    "\n",
    "# Total unique characters\n",
    "unique_chars = len(in_to_char)\n",
    "\n",
    "# Dictionary that maps characters to their respective indices\n",
    "char_to_in = {in_to_char[i]:i for i in range(len(in_to_char))}\n",
    "print(len(text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "epochs = 150000\n",
    "learning_rate = 1e-1\n",
    "seq_length = 25\n",
    "hidden_dim = 100\n",
    "display_step = 1000\n",
    "\n",
    "# Initializing the network to input and output\n",
    "# one-hot encoded indices of characters, and\n",
    "# training it to predict the next character in a\n",
    "# sequence.\n",
    "net = RecurrentNeuralNet(learning_rate, unique_chars, hidden_dim, unique_chars)\n",
    "\n",
    "# Initializing 'bookmark' in text file and setting\n",
    "# hidden state to 0\n",
    "text_index = 0\n",
    "prev_hidden = np.zeros((1, hidden_dim))\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    if text_index + seq_length + 1 >= len(text):\n",
    "        # Resetting 'bookmark' in text file and setting\n",
    "        # hidden state to 0, resetting memory\n",
    "        text_index = 0\n",
    "        prev_hidden = np.zeros((1, hidden_dim))\n",
    "\n",
    "    # Getting input as next 25 characters in text file, and setting output\n",
    "    # to the input offset by one. ie input: [0,1,2], output: [1,2,3] \n",
    "    input_text = [char_to_in[x] for x in text[text_index:text_index+seq_length]]\n",
    "    output_text = [char_to_in[x] for x in text[text_index+1:text_index+seq_length+1]]\n",
    "    \n",
    "    input_1hot = np.zeros((seq_length, unique_chars))\n",
    "    output_1hot = np.zeros((seq_length, unique_chars))\n",
    "    \n",
    "    # Converting indexes into one-hot\n",
    "    \n",
    "    for i in range(seq_length):\n",
    "        input_1hot[i][input_text[i]] = 1\n",
    "        output_1hot[i][output_text[i]] = 1\n",
    "        \n",
    "    # Updating sequence start index in text file\n",
    "    if not text_index + seq_length + 1 >= len(text):\n",
    "        text_index += seq_length\n",
    "        \n",
    "    # Performing adaptive gradient descent\n",
    "    gradients, prev_hidden = net.compute_gradients(input_1hot, output_1hot, prev_hidden)\n",
    "    net.apply_adagrad(gradients)\n",
    "    \n",
    "    # Displaying samples every 1000th iteration\n",
    "    if epoch % display_step == 0:\n",
    "        raw = net.sample(prev_hidden, input_text[0])\n",
    "        print(\"Epoch\", epoch)\n",
    "        print(\"\\n\\n\", \"\".join(in_to_char[i] for i in raw), \"\\n\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Printing out a test sample with 3000 characters\n",
    "\n",
    "sample = net.sample(np.zeros((1, hidden_dim)), char_to_in['I'], length=3000)\n",
    "print(\"\\n\\n\", \"\".join(in_to_char[i] for i in sample), \"\\n\\n\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
